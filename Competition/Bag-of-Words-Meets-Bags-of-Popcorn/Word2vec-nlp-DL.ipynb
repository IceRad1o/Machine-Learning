{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "train = pd.read_csv(\"../Dataset/word2vec-nlp/labeledTrainData.tsv\", header=0, delimiter='\\t')\n",
    "test = pd.read_csv(\"../Dataset/word2vec-nlp/testData.tsv\", header=0, delimiter='\\t')\n",
    "test[\"sentiment\"] = test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\n",
    "y_test = test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_text(review):\n",
    "    \"\"\"Return extracted text string from provided HTML string.\"\"\"\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    if len(review_text) == 0:\n",
    "        review_text = review\n",
    "    review_text = re.sub(r\"\\<.*\\>\", \"\", review_text)\n",
    "    try:\n",
    "        review_text = review_text.encode('ascii', 'ignore').decode('ascii')  # ignore \\xc3 etc.\n",
    "    except UnicodeDecodeError:\n",
    "        review_text = review_text.decode(\"ascii\", \"ignore\")\n",
    "    return review_text\n",
    "\n",
    "\n",
    "def letters_only(text):\n",
    "    \"\"\"Return input string with only letters (no punctuation, no numbers).\"\"\"\n",
    "    # It is probably worth experimenting with milder prepreocessing (eg just removing punctuation)\n",
    "    return re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "\n",
    "def rnn_tokenizer_review_preprocess(review):\n",
    "    \"\"\"Preprocessing used before fitting/transforming RNN tokenizer - Html->text, remove punctuation/#s, lowercase.\"\"\"\n",
    "    return letters_only(html_to_text(review)).lower()\n",
    "\n",
    "\n",
    "def get_train_val_data(reviews_to_features_fn=None, df=train):\n",
    "    \"\"\"Extracts features (using reviews_to_features_fn), splits into train/test data, and returns\n",
    "    x_train, y_train, x_test, y_test.  If no feature extraction function is provided, x_train/x_test will\n",
    "    simply consist of a Series of all the reviews.\n",
    "    \"\"\"\n",
    "    #     df = pd.read_csv('labeledTrainData.tsv', header=0, quotechar='\"', sep='\\t')\n",
    "    SEED = 1000\n",
    "    # Shuffle data frame rows\n",
    "    np.random.seed(SEED)\n",
    "    df = df.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "    if reviews_to_features_fn:\n",
    "        feature_rows = df[\"review\"].map(reviews_to_features_fn)\n",
    "        if type(feature_rows[0]) == np.ndarray:\n",
    "            num_instances = len(feature_rows)\n",
    "            num_features = len(feature_rows[0])\n",
    "            x = np.concatenate(feature_rows.values).reshape((num_instances, num_features))\n",
    "        else:\n",
    "            x = feature_rows\n",
    "    else:\n",
    "        x = df[\"review\"]\n",
    "\n",
    "    y = df[\"sentiment\"]\n",
    "\n",
    "    # Split 80/20\n",
    "    test_start_index = int(df.shape[0] * .8)\n",
    "    x_train = x[0:test_start_index]\n",
    "    y_train = y[0:test_start_index]\n",
    "    x_val = x[test_start_index:]\n",
    "    y_val = y[test_start_index:]\n",
    "\n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x_train, y_train, x_val, y_val = get_train_val_data(rnn_tokenizer_review_preprocess)\n",
    "\n",
    "x_test = test[\"review\"].map(rnn_tokenizer_review_preprocess)\n",
    "y_test = test[\"sentiment\"]\n",
    "\n",
    "# ----- lex -----\n",
    "\n",
    "pos_list = []\n",
    "neg_list = []\n",
    "with open(\"../Dataset/SentiWordNet/SentiWordNet_3.0.0.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if line[0] != '#':\n",
    "            # print(line)\n",
    "            strarr = line.split('\\t')\n",
    "            # print(strarr)\n",
    "            # print(strarr[2].split(\"\\n\")[0])\n",
    "            if (strarr[2] != '') & (strarr[2] != ' ') & (strarr[2] != '#'):\n",
    "                if float(strarr[2].split(\"\\n\")[0]) > float(strarr[3].split(\"\\n\")[0]):\n",
    "                    pos_list += [strarr[4].split('#')[0]]\n",
    "                if float(strarr[2].split(\"\\n\")[0]) < float(strarr[3].split(\"\\n\")[0]):\n",
    "                    neg_list += [strarr[4].split('#')[0]]\n",
    "\n",
    "# print(pos_list)\n",
    "# print(neg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost: 52.7130651473999\n",
      "Prediction:\n",
      "51\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "pos = 0\n",
    "neg = 0\n",
    "\n",
    "pre = -1\n",
    "# print(len(x_test))\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def comp(str, pos_list, neg_list, pos, neg):\n",
    "    if str in pos_list:\n",
    "        pos += 1\n",
    "    if str in neg_list:\n",
    "        neg += 1\n",
    "    return\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# for index in range(1000):\n",
    "#     str_arr = x_test[index].split(' ')\n",
    "#     Parallel(n_jobs=4)(delayed(comp)(str,pos_list,neg_list,pos,neg) for str in str_arr)\n",
    "#     if pos >= neg:\n",
    "#         pre = 1\n",
    "#     else:\n",
    "#         pre = 0\n",
    "#     pos=0\n",
    "#     neg=0\n",
    "#     if pre == y_test[index]:\n",
    "#         predict_correct+=1\n",
    "#     else:\n",
    "#         predict_error+=1\n",
    "predict_correct = 0\n",
    "predict_error = 0\n",
    "\n",
    "def par_comp(mark, index):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    str_arr = x_test[index].split(' ')\n",
    "    for i in str_arr:\n",
    "        if i in pos_list:\n",
    "            pos += 1\n",
    "        if i in neg_list:\n",
    "            neg += 1\n",
    "    if pos >= neg:\n",
    "        pre = 1\n",
    "    else:\n",
    "        pre = 0\n",
    "\n",
    "    if pre == y_test[index]:\n",
    "        mark += 1\n",
    "    else:\n",
    "        mark -= 1\n",
    "\n",
    "    return mark\n",
    "\n",
    "mark = 0\n",
    "mark = Parallel(n_jobs=2)(\n",
    "    delayed\n",
    "    (par_comp)\n",
    "    (mark, index)\n",
    "    for index\n",
    "    in range(100))\n",
    "\n",
    "\n",
    "#\n",
    "# for index in range(1000):\n",
    "#     str_arr = x_test[index].split(' ')\n",
    "#     for i in str_arr:\n",
    "#         if i in pos_list:\n",
    "#             pos+=1\n",
    "#         if i in neg_list:\n",
    "#             neg+=1\n",
    "#     if pos >= neg:\n",
    "#         pre = 1\n",
    "#     else:\n",
    "#         pre = 0\n",
    "#     pos=0\n",
    "#     neg=0\n",
    "#     if pre == y_test[index]:\n",
    "#         predict_correct+=1\n",
    "#     else:\n",
    "#         predict_error+=1\n",
    "print(\"time cost:\", time.time() - start_time)\n",
    "for i in mark:\n",
    "    if i == 1:\n",
    "        predict_correct+=1\n",
    "    else:\n",
    "        predict_error+=1\n",
    "print(\"Prediction:\")\n",
    "print(predict_correct)\n",
    "print(predict_error)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(pos)\n",
    "# print(neg)\n",
    "# print(y_test[0])\n",
    "\n",
    "\n",
    "#\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "#\n",
    "#\n",
    "# np.random.seed(1000)\n",
    "# num_most_freq_words_to_include = 5000\n",
    "# MAX_REVIEW_LENGTH_FOR_KERAS_RNN = 500\n",
    "# embedding_vector_length = 32\n",
    "# # train_review_list = [s.encode('ascii') for s in x_train.tolist()]\n",
    "# # val_review_list = [s.encode('ascii') for s in x_val.tolist()]\n",
    "# # all_review_list = train_review_list + val_review_list\n",
    "# train_review_list = x_train.tolist()\n",
    "# val_review_list = x_val.tolist()\n",
    "# test_review_list = x_test.tolist()\n",
    "# all_review_list = x_train.tolist() + x_val.tolist()\n",
    "# tokenizer = Tokenizer(num_words=num_most_freq_words_to_include)\n",
    "# tokenizer.fit_on_texts(all_review_list)\n",
    "# train_reviews_tokenized = tokenizer.texts_to_sequences(train_review_list)\n",
    "# x_train = pad_sequences(train_reviews_tokenized, maxlen=MAX_REVIEW_LENGTH_FOR_KERAS_RNN)\n",
    "# val_review_tokenized = tokenizer.texts_to_sequences(val_review_list)\n",
    "# x_val = pad_sequences(val_review_tokenized, maxlen=MAX_REVIEW_LENGTH_FOR_KERAS_RNN)\n",
    "# test_review_tokenized = tokenizer.texts_to_sequences(test_review_list)\n",
    "# x_test = pad_sequences(test_review_tokenized, maxlen=MAX_REVIEW_LENGTH_FOR_KERAS_RNN)\n",
    "\n",
    "#\n",
    "# from keras.layers import Input, Embedding, Dropout, Conv1D, MaxPool1D, GRU, LSTM, Dense\n",
    "# from keras.models import Model\n",
    "#\n",
    "#\n",
    "# def rnn_model(use_cnn=True, use_lstm=False):\n",
    "#     input_sequences = Input(shape=(MAX_REVIEW_LENGTH_FOR_KERAS_RNN,))\n",
    "#     initial_dropout = 0.2\n",
    "#     embedding_layer = Embedding(input_dim=num_most_freq_words_to_include,\n",
    "#                                 output_dim=embedding_vector_length,\n",
    "#                                 input_length=MAX_REVIEW_LENGTH_FOR_KERAS_RNN)\n",
    "#     X = embedding_layer(input_sequences)\n",
    "#     X = Dropout(0.2)(X)\n",
    "#     if use_cnn:\n",
    "#         X = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(X)\n",
    "#         X = MaxPool1D(pool_size=2)(X)\n",
    "#\n",
    "#     # Add GRU layers\n",
    "#     dropout_W = 0.0\n",
    "#     dropout_U = 0.0\n",
    "#\n",
    "#     if use_lstm:\n",
    "#         X = LSTM(100, dropout=dropout_W, recurrent_dropout=dropout_U)(X)\n",
    "#     else:\n",
    "#         X = GRU(100, dropout=dropout_W, recurrent_dropout=dropout_U)(X)\n",
    "#     X = Dropout(0.2)(X)\n",
    "#     outputs = Dense(1, activation='sigmoid')(X)\n",
    "#     model = Model(inputs=input_sequences, outputs=outputs)\n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#\n",
    "#     return model\n",
    "#\n",
    "#\n",
    "#\n",
    "# gru_model = rnn_model(use_lstm=False)\n",
    "# gru_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
